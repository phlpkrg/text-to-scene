{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code used for fine-tuning of T5-Small models in Colab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from google.colab import drive\n",
    "\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "# === Paths ===\n",
    "BASE_PATH = Path(\"/content/drive/My Drive/...\")\n",
    "MODEL_PATH = BASE_PATH / \"model\"\n",
    "RESULTS_PATH = BASE_PATH / \"results\"\n",
    "LOGS_PATH = BASE_PATH / \"logs\"\n",
    "DATA_PATH = Path(\"derived_dataset.json\")\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"Loads input-output pairs from JSON file.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "    return [item[\"input\"] for item in data], [item[\"output\"] for item in data]\n",
    "\n",
    "\n",
    "def preprocess_data(batch, tokenizer):\n",
    "    \"\"\"Tokenizes input and output pairs.\"\"\"\n",
    "    inputs = tokenizer(batch[\"input\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    outputs = tokenizer(batch[\"output\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": inputs[\"input_ids\"],\n",
    "        \"attention_mask\": inputs[\"attention_mask\"],\n",
    "        \"labels\": outputs[\"input_ids\"]\n",
    "    }\n",
    "\n",
    "\n",
    "def prepare_datasets(inputs, outputs, tokenizer):\n",
    "    \"\"\"Creates and tokenizes Hugging Face Datasets.\"\"\"\n",
    "    dataset = Dataset.from_dict({\"input\": inputs, \"output\": outputs})\n",
    "    split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "    train_dataset = split[\"train\"].map(lambda x: preprocess_data(x, tokenizer), batched=True, remove_columns=[\"input\", \"output\"])\n",
    "    val_dataset = split[\"test\"].map(lambda x: preprocess_data(x, tokenizer), batched=True, remove_columns=[\"input\", \"output\"])\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "\n",
    "def configure_training_args():\n",
    "    \"\"\"Defines training configuration.\"\"\"\n",
    "    return TrainingArguments(\n",
    "        output_dir=str(RESULTS_PATH),\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=3000,\n",
    "        save_steps=3000,\n",
    "        save_strategy=\"steps\",\n",
    "        logging_dir=str(LOGS_PATH),\n",
    "        logging_steps=100,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=16,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        fp16=True,\n",
    "        report_to=\"tensorboard\",\n",
    "        load_best_model_at_end=True,\n",
    "        gradient_accumulation_steps=4\n",
    "    )\n",
    "\n",
    "\n",
    "def train_model():\n",
    "    \"\"\"Runs full fine-tuning pipeline for T5.\"\"\"\n",
    "    inputs, outputs = load_data(DATA_PATH)\n",
    "\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "    train_dataset, val_dataset = prepare_datasets(inputs, outputs, tokenizer)\n",
    "    training_args = configure_training_args()\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    model.save_pretrained(MODEL_PATH)\n",
    "    tokenizer.save_pretrained(MODEL_PATH)\n",
    "\n",
    "    print(f\"Model saved to: {MODEL_PATH}\")\n",
    "    print(f\"Checkpoints stored in: {RESULTS_PATH}\")\n",
    "    print(f\"Logs stored in: {LOGS_PATH}\")\n",
    "\n",
    "\n",
    "# === Entry Point ===\n",
    "train_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
