{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Run the Benchmark Test For a Language Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step01 - Copy Validation dataset (benchmark_test.json)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"benchmark_test.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "with open(\"step01.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(data, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step02 - Run LLM Architecture on dataset inputs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to use the correct variant for single-model vs. two-model systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Use for SINGLE-MODEL VERSION ===\n",
    "\n",
    "import json\n",
    "import re\n",
    "import ast\n",
    "import logging\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# === Logging Configuration ===\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# === Load Model and Tokenizer ===\n",
    "llm0_tokenizer = T5Tokenizer.from_pretrained(\"./llm0_v1\")\n",
    "llm0_model = T5ForConditionalGeneration.from_pretrained(\"./llm0_v1\")\n",
    "\n",
    "\n",
    "# === LLM Helpers ===\n",
    "\n",
    "def extract_scaling(prompt):\n",
    "    \"\"\"Extracts the scaling value and returns (key, value, cleaned prompt).\"\"\"\n",
    "    match = re.search(r\"(scaling)\\s*=\\s*([0-1](?:\\.\\d+)?)\", prompt, re.IGNORECASE)\n",
    "    if match:\n",
    "        value = float(match.group(2))\n",
    "        cleaned = re.sub(r\"(scaling)\\s*=\\s*([0-1](?:\\.\\d+)?)(,)?\", \"\", prompt, flags=re.IGNORECASE).strip().rstrip(\",\")\n",
    "        return match.group(1), value, cleaned\n",
    "    return None, 0.0, prompt\n",
    "\n",
    "\n",
    "def generate_text(model, tokenizer, query):\n",
    "    \"\"\"Generates text from a model given an input query string.\"\"\"\n",
    "    input_ids = tokenizer.encode(query, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_length=512,\n",
    "        num_beams=5,\n",
    "        temperature=0.3,\n",
    "        top_k=50,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# === Parsing Helpers ===\n",
    "\n",
    "def parse_llm0_output(output_str):\n",
    "    \"\"\"Parses the output string into rooms, connections, and placements.\"\"\"\n",
    "    try:\n",
    "        parsed = ast.literal_eval(\"{\" + output_str + \"}\")\n",
    "    except (SyntaxError, ValueError) as e:\n",
    "        raise ValueError(f\"Invalid input format: {e}\")\n",
    "\n",
    "    rooms = parsed.get(\"rooms\", [])\n",
    "    connections = parsed.get(\"connections\", [])\n",
    "    raw_assets = parsed.get(\"assets\", [])\n",
    "\n",
    "    placements = {}\n",
    "    for room, asset in raw_assets:\n",
    "        if room in placements:\n",
    "            placements[room].append(asset)\n",
    "        else:\n",
    "            placements[room] = [asset]\n",
    "\n",
    "    return rooms, connections, placements\n",
    "\n",
    "\n",
    "def case_shifter(asset, reference_list):\n",
    "    \"\"\"Matches the asset to reference list regardless of case/spacing.\"\"\"\n",
    "    formatted = asset.replace(\" \", \"\").lower()\n",
    "    for ref in reference_list:\n",
    "        if formatted == ref.replace(\" \", \"\").lower():\n",
    "            return ref\n",
    "    return \"Unknown\"\n",
    "\n",
    "\n",
    "def build_json(rooms, connections, placements):\n",
    "    \"\"\"Builds the final structured output as a JSON string.\"\"\"\n",
    "    indexed = {i: placements.get(room, []) for i, room in enumerate(rooms)}\n",
    "    return json.dumps({\n",
    "        \"connections\": connections,\n",
    "        \"room names\": [room.rsplit(\" \", 1)[0] for room in rooms],\n",
    "        \"rooms\": indexed\n",
    "    }, indent=4)\n",
    "\n",
    "\n",
    "# === Inference ===\n",
    "\n",
    "def infer(query):\n",
    "    \"\"\"Main inference pipeline using only LLM0 (single-model approach).\"\"\"\n",
    "    _, scaling, user_prompt = extract_scaling(query)\n",
    "    logging.info(f\"Scaling factor: {scaling}\")\n",
    "\n",
    "    match = re.search(r\"prompt='(.*?)'\", user_prompt)\n",
    "    prompt = match.group(1) if match else \"\"\n",
    "\n",
    "    assets = []\n",
    "    available_match = re.search(r\"available=\\[(.*?)\\]\", user_prompt)\n",
    "    if available_match:\n",
    "        assets = ast.literal_eval(f\"[{available_match.group(1)}]\")\n",
    "        assets = [a.lower() for a in assets]\n",
    "\n",
    "    llm0_query = f\"prompt='{prompt}', available={assets}\"\n",
    "    llm0_output = generate_text(llm0_model, llm0_tokenizer, llm0_query)\n",
    "    print(llm0_output)\n",
    "\n",
    "    rooms, connections, placements = parse_llm0_output(llm0_output)\n",
    "\n",
    "    return build_json(rooms, connections, placements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Use for TWO-MODEL VERSION ===\n",
    "\n",
    "import json\n",
    "import re\n",
    "import ast\n",
    "import logging\n",
    "import random\n",
    "import concurrent.futures\n",
    "from pathlib import Path\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# === Logging Configuration ===\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# === Load Models and Tokenizers ===\n",
    "llm0_tokenizer = T5Tokenizer.from_pretrained(\"./llm0_single\")\n",
    "llm0_model = T5ForConditionalGeneration.from_pretrained(\"./llm0_single\")\n",
    "\n",
    "llm1_tokenizer = T5Tokenizer.from_pretrained(\"./llm1_single\")\n",
    "llm1_model = T5ForConditionalGeneration.from_pretrained(\"./llm1_single\")\n",
    "\n",
    "llm2_tokenizer = T5Tokenizer.from_pretrained(\"./llm2_v111\")\n",
    "llm2_model = T5ForConditionalGeneration.from_pretrained(\"./llm2_v111\")\n",
    "\n",
    "\n",
    "# === File Helpers ===\n",
    "\n",
    "def load_dataset(path):\n",
    "    \"\"\"Loads a JSON file and returns the data.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "\n",
    "def save_dataset(path, data):\n",
    "    \"\"\"Saves the dataset to the given path.\"\"\"\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "    print(f\"Saved {path.name} with {len(data)} entries.\")\n",
    "\n",
    "\n",
    "# === LLM Helpers ===\n",
    "\n",
    "def extract_scaling(prompt):\n",
    "    \"\"\"Extracts the scaling value and returns (key, value, cleaned prompt).\"\"\"\n",
    "    match = re.search(r\"(scaling)\\s*=\\s*([0-1](?:\\.\\d+)?)\", prompt, re.IGNORECASE)\n",
    "    if match:\n",
    "        value = float(match.group(2))\n",
    "        cleaned = re.sub(r\"(scaling)\\s*=\\s*([0-1](?:\\.\\d+)?)(,)?\", \"\", prompt, flags=re.IGNORECASE).strip().rstrip(\",\")\n",
    "        return match.group(1), value, cleaned\n",
    "    return None, 0.0, prompt\n",
    "\n",
    "\n",
    "def generate_text(model, tokenizer, query):\n",
    "    \"\"\"Generates text from a model given an input query string.\"\"\"\n",
    "    input_ids = tokenizer.encode(query, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_length=512,\n",
    "        num_beams=5,\n",
    "        temperature=0.3,\n",
    "        top_k=50,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# === Parsing Helpers ===\n",
    "\n",
    "def parse_llm0_output(output_str):\n",
    "    \"\"\"Parses the room list from LLM0 output.\"\"\"\n",
    "    return ast.literal_eval(output_str)\n",
    "\n",
    "\n",
    "def parse_llm1_output(output_str, rooms):\n",
    "    \"\"\"Parses the connection list from LLM1 output based on room indexes.\"\"\"\n",
    "    raw = ast.literal_eval(output_str)\n",
    "    return [[rooms.index(room) for room in group] for group in raw]\n",
    "\n",
    "\n",
    "def parse_llm2_output(output_str, available_assets):\n",
    "    \"\"\"Parses asset placement output from LLM2.\"\"\"\n",
    "    data = {}\n",
    "    entries = re.split(r',\\s*(?![^\\[]*\\])', output_str)\n",
    "    for entry in entries:\n",
    "        key, value = entry.split(\"=\", 1)\n",
    "        key = key.strip()\n",
    "        value = eval(value.strip())\n",
    "        data[key] = [case_shifter(asset, available_assets) for asset in value]\n",
    "    return data\n",
    "\n",
    "\n",
    "def case_shifter(asset, reference_list):\n",
    "    \"\"\"Matches the asset to reference list regardless of case/spacing.\"\"\"\n",
    "    formatted = asset.replace(\" \", \"\").lower()\n",
    "    for ref in reference_list:\n",
    "        if formatted == ref.replace(\" \", \"\").lower():\n",
    "            return ref\n",
    "    return \"Unknown\"\n",
    "\n",
    "\n",
    "def build_json(rooms, connections, placements):\n",
    "    \"\"\"Builds the final structured output as a JSON string.\"\"\"\n",
    "    indexed = {i: placements.get(room, []) for i, room in enumerate(rooms)}\n",
    "    return json.dumps({\n",
    "        \"connections\": connections,\n",
    "        \"room names\": [room.rsplit(\" \", 1)[0] for room in rooms],\n",
    "        \"rooms\": indexed\n",
    "    }, indent=4)\n",
    "\n",
    "\n",
    "# === Inference ===\n",
    "\n",
    "def infer(query):\n",
    "    \"\"\"Main inference pipeline using LLM0 → LLM1 + LLM2.\"\"\"\n",
    "    _, scaling, user_prompt = extract_scaling(query)\n",
    "    logging.info(f\"Scaling factor: {scaling}\")\n",
    "\n",
    "    match = re.search(r\"prompt='(.*?)'\", user_prompt)\n",
    "    prompt = match.group(1) if match else \"\"\n",
    "\n",
    "    assets = []\n",
    "    available_match = re.search(r\"available=\\[(.*?)\\]\", user_prompt)\n",
    "    if available_match:\n",
    "        assets = ast.literal_eval(f\"[{available_match.group(1)}]\")\n",
    "        assets = [a.lower() for a in assets]\n",
    "\n",
    "    # LLM0\n",
    "    llm0_out = generate_text(llm0_model, llm0_tokenizer, prompt)\n",
    "    rooms = parse_llm0_output(llm0_out)\n",
    "\n",
    "    # LLM1 and LLM2 in parallel\n",
    "    llm1_query = f\"prompt='{prompt}', rooms='{llm0_out}'\"\n",
    "    llm2_query = f\"prompt='{prompt}', rooms={str(rooms)}, available={assets}\"\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future1 = executor.submit(generate_text, llm1_model, llm1_tokenizer, llm1_query)\n",
    "        future2 = executor.submit(generate_text, llm2_model, llm2_tokenizer, llm2_query)\n",
    "        llm1_out = future1.result()\n",
    "        llm2_out = future2.result()\n",
    "\n",
    "    connections = parse_llm1_output(llm1_out, rooms)\n",
    "    placements = parse_llm2_output(llm2_out, assets)\n",
    "\n",
    "    return build_json(rooms, connections, placements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# === File Paths ===\n",
    "DATASET_PATH = Path(\"step01.json\")\n",
    "\n",
    "\n",
    "def load_dataset(path):\n",
    "    \"\"\"Loads a JSON file and returns the dataset.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "\n",
    "def save_dataset(path, data):\n",
    "    \"\"\"Saves the dataset to the specified path.\"\"\"\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "    print(f\"Saved {path.name} with {len(data)} entries.\")\n",
    "\n",
    "\n",
    "def process_entries(path):\n",
    "    \"\"\"Processes each entry by running inference if not already done.\"\"\"\n",
    "    dataset = load_dataset(path)\n",
    "\n",
    "    for index, entry in enumerate(dataset):\n",
    "        if \"generated_output\" in entry:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            result = infer(entry[\"input\"])\n",
    "            entry[\"generated_output\"] = result\n",
    "            save_dataset(path, dataset)\n",
    "            print(f\"Processed entry {index + 1}/{len(dataset)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing entry {index + 1}: {e}\")\n",
    "\n",
    "    print(\"Processing complete. Updated dataset saved.\")\n",
    "\n",
    "\n",
    "# === Run ===\n",
    "process_entries(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step03 - Run Validation Part 1 (Room Numbers, Connection Type)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "\n",
    "def load_dataset(path):\n",
    "    \"\"\"Loads a JSON file and returns the dataset.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "\n",
    "def save_dataset(path, data):\n",
    "    \"\"\"Saves the dataset to the given path.\"\"\"\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "    print(f\"Saved {path.name} with {len(data)} entries.\")\n",
    "\n",
    "\n",
    "# === Connection Type Checks ===\n",
    "\n",
    "def is_fully_connected(n, connections):\n",
    "    \"\"\"Returns True if every room is connected to every other room.\"\"\"\n",
    "    expected = set((i, j) for i in range(n) for j in range(i + 1, n))\n",
    "    return set(map(tuple, connections)) == expected\n",
    "\n",
    "\n",
    "def is_circular_connected(n, connections):\n",
    "    \"\"\"Returns True if rooms form a circular layout.\"\"\"\n",
    "    if n == 1:\n",
    "        return True\n",
    "    if len(connections) != n:\n",
    "        return False\n",
    "\n",
    "    graph = defaultdict(set)\n",
    "    for a, b in connections:\n",
    "        graph[a].add(b)\n",
    "        graph[b].add(a)\n",
    "\n",
    "    visited = set()\n",
    "    queue = deque([0])\n",
    "\n",
    "    while queue:\n",
    "        node = queue.popleft()\n",
    "        if node in visited:\n",
    "            continue\n",
    "        visited.add(node)\n",
    "        queue.extend(graph[node] - visited)\n",
    "\n",
    "    return len(visited) == n and all(len(graph[node]) == 2 for node in graph)\n",
    "\n",
    "\n",
    "def is_linear_connected(n, connections):\n",
    "    \"\"\"Returns True if rooms form a single straight line.\"\"\"\n",
    "    if n == 1:\n",
    "        return True\n",
    "    if len(connections) != n - 1:\n",
    "        return False\n",
    "\n",
    "    graph = defaultdict(set)\n",
    "    for a, b in connections:\n",
    "        graph[a].add(b)\n",
    "        graph[b].add(a)\n",
    "\n",
    "    endpoints = [node for node in graph if len(graph[node]) == 1]\n",
    "    if len(endpoints) != 2:\n",
    "        return False\n",
    "\n",
    "    visited = set()\n",
    "    queue = deque([endpoints[0]])\n",
    "\n",
    "    while queue:\n",
    "        node = queue.popleft()\n",
    "        if node in visited:\n",
    "            continue\n",
    "        visited.add(node)\n",
    "        queue.extend(graph[node] - visited)\n",
    "\n",
    "    return len(visited) == n\n",
    "\n",
    "\n",
    "def is_central_to_room(n, connections, room_names, connection_type):\n",
    "    \"\"\"Checks if all rooms connect to one specific room.\"\"\"\n",
    "    if n == 1:\n",
    "        return True\n",
    "\n",
    "    room_counts = {}\n",
    "    for a, b in connections:\n",
    "        room_counts[a] = room_counts.get(a, 0) + 1\n",
    "        room_counts[b] = room_counts.get(b, 0) + 1\n",
    "\n",
    "    central_rooms = [room for room, count in room_counts.items() if count == n - 1]\n",
    "\n",
    "    if connection_type == \"central to ANY\":\n",
    "        return len(central_rooms) > 0\n",
    "\n",
    "    expected_central = connection_type.replace(\"central to \", \"\").strip()\n",
    "    return any(\n",
    "        0 <= room < len(room_names) and room_names[room] == expected_central\n",
    "        for room in central_rooms\n",
    "    )\n",
    "\n",
    "\n",
    "# === Main Evaluation ===\n",
    "\n",
    "def validate_connections(entry):\n",
    "    \"\"\"Validates the connection type and room count in the entry.\"\"\"\n",
    "    output = json.loads(entry[\"generated_output\"])\n",
    "    room_names = output[\"room names\"]\n",
    "    num_rooms = len(room_names)\n",
    "\n",
    "    entry[\"number_rooms_correct\"] = num_rooms == entry[\"number_rooms\"]\n",
    "    connections = output.get(\"connections\", [])\n",
    "    connection_type = entry[\"connection_type\"]\n",
    "\n",
    "    if connection_type == \"fully connected\":\n",
    "        entry[\"connection_type_correct\"] = is_fully_connected(num_rooms, connections)\n",
    "    elif connection_type == \"circular connected\":\n",
    "        entry[\"connection_type_correct\"] = is_circular_connected(num_rooms, connections)\n",
    "    elif connection_type == \"linear connected\":\n",
    "        entry[\"connection_type_correct\"] = is_linear_connected(num_rooms, connections)\n",
    "    elif connection_type.startswith(\"central to\"):\n",
    "        entry[\"connection_type_correct\"] = is_central_to_room(num_rooms, connections, room_names, connection_type)\n",
    "    else:\n",
    "        entry[\"connection_type_correct\"] = True\n",
    "\n",
    "    return entry\n",
    "\n",
    "\n",
    "def process_file(input_file, output_file):\n",
    "    \"\"\"Main processing function to validate connection types and update entries.\"\"\"\n",
    "    input_path = Path(input_file)\n",
    "    output_path = Path(output_file)\n",
    "\n",
    "    dataset = load_dataset(input_path)\n",
    "    updated = [validate_connections(entry) for entry in dataset]\n",
    "    save_dataset(output_path, updated)\n",
    "\n",
    "\n",
    "# === Entry Point ===\n",
    "INPUT_FILE = \"step02.json\"\n",
    "OUTPUT_FILE = \"step03.json\"\n",
    "process_file(INPUT_FILE, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step04 - Run Validation Part 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def load_dataset(path):\n",
    "    \"\"\"Loads a JSON file and returns the data.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "\n",
    "def save_dataset(path, data):\n",
    "    \"\"\"Saves the dataset to the given path.\"\"\"\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "    print(f\"Saved {path.name} with {len(data)} entries.\")\n",
    "\n",
    "\n",
    "def map_connection_indexes_to_names(connection, room_names):\n",
    "    \"\"\"Convert a connection tuple of indexes to corresponding room names.\"\"\"\n",
    "    return tuple(room_names[idx].lower() for idx in connection if idx < len(room_names))\n",
    "\n",
    "\n",
    "def validate_entries(dataset):\n",
    "    \"\"\"Validates each entry for rooms, connections, and asset placement.\"\"\"\n",
    "    for entry in dataset:\n",
    "        assets_required = entry.get(\"assets\", [])\n",
    "        include_rooms = [room.lower() for room in entry.get(\"include_rooms\", [])]\n",
    "        exclude_rooms = [room.lower() for room in entry.get(\"exclude_rooms\", [])]\n",
    "        include_connections = entry.get(\"include_connections\", [])\n",
    "        exclude_connections = entry.get(\"exclude_connections\", [])\n",
    "        generated_output = json.loads(entry.get(\"generated_output\", \"{}\"))\n",
    "        input_text = entry.get(\"input\", \"\")\n",
    "\n",
    "        # Extract available assets from input\n",
    "        available_match = re.search(r\"available=\\[(.*?)\\]\", input_text)\n",
    "        if available_match:\n",
    "            available_items = {item.strip().strip(\"'\").lower() for item in available_match.group(1).split(\",\")}\n",
    "        else:\n",
    "            available_items = set()\n",
    "\n",
    "        room_names = generated_output.get(\"room names\", [])\n",
    "        rooms = generated_output.get(\"rooms\", {})\n",
    "        connections = generated_output.get(\"connections\", [])\n",
    "\n",
    "        room_index_map = {str(idx): name.lower() for idx, name in enumerate(room_names)}\n",
    "\n",
    "        # Rebuild room: [assets] dictionary with lowercase\n",
    "        generated_assets = {}\n",
    "        for idx, assets in rooms.items():\n",
    "            room_name = room_index_map.get(idx, \"\")\n",
    "            if room_name:\n",
    "                generated_assets.setdefault(room_name, []).extend([asset.lower() for asset in assets])\n",
    "\n",
    "        # Normalize required assets\n",
    "        assets_required_lower = [(room.lower(), asset.lower()) for room, asset in assets_required]\n",
    "\n",
    "        items_correct = []\n",
    "        items_missing = []\n",
    "        items_over = []\n",
    "        asset_satisfied = {}\n",
    "\n",
    "        for room_type, asset in assets_required_lower:\n",
    "            check_asset = \"unknown\" if asset not in available_items else asset\n",
    "\n",
    "            if room_type == \"somewhere\":\n",
    "                found = any(check_asset in assets for assets in generated_assets.values())\n",
    "            else:\n",
    "                found = any(check_asset in assets for room, assets in generated_assets.items() if room_type in room)\n",
    "\n",
    "            if found:\n",
    "                asset_satisfied[(room_type, check_asset)] = True\n",
    "                items_correct.append([room_type, check_asset])\n",
    "            else:\n",
    "                asset_satisfied.setdefault((room_type, check_asset), False)\n",
    "\n",
    "        for (room_type, check_asset), satisfied in asset_satisfied.items():\n",
    "            if not satisfied:\n",
    "                items_missing.append([room_type, check_asset])\n",
    "\n",
    "        required_assets_set = {\n",
    "            (room, \"unknown\" if asset not in available_items else asset)\n",
    "            for room, asset in assets_required_lower\n",
    "        }\n",
    "\n",
    "        for room, assets in generated_assets.items():\n",
    "            for asset in assets:\n",
    "                check_asset = \"unknown\" if asset not in available_items else asset\n",
    "                if (room, check_asset) not in required_assets_set and (\"somewhere\", check_asset) not in required_assets_set:\n",
    "                    items_over.append([room, check_asset])\n",
    "\n",
    "        generated_rooms_set = set(generated_assets.keys())\n",
    "\n",
    "        rooms_correct = [room for room in include_rooms if room in generated_rooms_set]\n",
    "        rooms_missing = [room for room in include_rooms if room not in generated_rooms_set]\n",
    "        rooms_over = [room for room in exclude_rooms if room in generated_rooms_set]\n",
    "\n",
    "        # Map connections by room names\n",
    "        generated_conn_names = {\n",
    "            map_connection_indexes_to_names(conn, room_names)\n",
    "            for conn in connections\n",
    "        }\n",
    "        expected_conn_names = {tuple(map(str.lower, conn)) for conn in include_connections}\n",
    "        excluded_conn_names = {tuple(map(str.lower, conn)) for conn in exclude_connections}\n",
    "\n",
    "        connections_correct = [conn for conn in expected_conn_names if conn in generated_conn_names]\n",
    "        connections_missing = [conn for conn in expected_conn_names if conn not in generated_conn_names]\n",
    "        connections_over = [conn for conn in excluded_conn_names if conn in generated_conn_names]\n",
    "\n",
    "        # Update entry\n",
    "        entry[\"items_correct\"] = items_correct\n",
    "        entry[\"items_missing\"] = items_missing\n",
    "        entry[\"items_over\"] = items_over\n",
    "        entry[\"rooms_correct\"] = rooms_correct\n",
    "        entry[\"rooms_missing\"] = rooms_missing\n",
    "        entry[\"rooms_over\"] = rooms_over\n",
    "        entry[\"connections_correct\"] = connections_correct\n",
    "        entry[\"connections_missing\"] = connections_missing\n",
    "        entry[\"connections_over\"] = connections_over\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def process_file(input_file, output_file):\n",
    "    \"\"\"Main processing function to validate asset, room, and connection correctness.\"\"\"\n",
    "    input_path = Path(input_file)\n",
    "    output_path = Path(output_file)\n",
    "\n",
    "    dataset = load_dataset(input_path)\n",
    "    validated = validate_entries(dataset)\n",
    "    save_dataset(output_path, validated)\n",
    "\n",
    "\n",
    "# === Entry Point ===\n",
    "INPUT_FILE = \"step03.json\"\n",
    "OUTPUT_FILE = \"step04.json\"\n",
    "process_file(INPUT_FILE, OUTPUT_FILE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
