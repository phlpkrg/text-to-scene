{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Base Dataset Generation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step01 - Generate Randomized Output Representations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "\n",
    "# Constants\n",
    "AIRCHITECT_ROOMS = [\n",
    "    \"Living Room\", \"Kitchen\", \"Bedroom\", \"Bathroom\", \"Dining Room\",\n",
    "    \"Study Room\", \"Storage\", \"Hallway\"\n",
    "]\n",
    "AIRCHITECT_ASSETS = [\n",
    "    \"Window\", \"Tv Cabinet\", \"Chair\", \"Bed\", \"Wardrobe\",\n",
    "    \"Shower\", \"Sink\", \"Toilet\", \"Sofa\", \"Table\"\n",
    "]\n",
    "\n",
    "\n",
    "def generate_random_connected_graph(num_nodes):\n",
    "    \"\"\"Generates a random connected graph (spanning tree only)\"\"\"\n",
    "    if num_nodes < 1:\n",
    "        raise ValueError(\"Number of nodes must be at least 1\")\n",
    "\n",
    "    graph = nx.Graph()\n",
    "    graph.add_nodes_from(range(num_nodes))\n",
    "\n",
    "    nodes = list(range(num_nodes))\n",
    "    random.shuffle(nodes)\n",
    "\n",
    "    for i in range(1, num_nodes):\n",
    "        parent = random.choice(nodes[:i])\n",
    "        graph.add_edge(parent, nodes[i])\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "def get_room_list(n_rooms, ask_for_rooms):\n",
    "    \"\"\"Returns a list of rooms based on preferences and randomness\"\"\"\n",
    "    if ask_for_rooms:\n",
    "        return random.choices(AIRCHITECT_ROOMS, k=n_rooms)\n",
    "\n",
    "    base_rooms = [\"Living Room\", \"Bedroom\"]\n",
    "    room_pool = AIRCHITECT_ROOMS.copy()\n",
    "\n",
    "    if n_rooms == 1:\n",
    "        return random.sample(base_rooms, 1)\n",
    "\n",
    "    rooms = random.sample(base_rooms, min(2, n_rooms))\n",
    "    rooms.append(\"Bathroom\")\n",
    "\n",
    "    remaining = n_rooms - len(rooms)\n",
    "    if remaining > 0:\n",
    "        extra_rooms = random.sample(room_pool, k=remaining)\n",
    "        rooms += extra_rooms\n",
    "\n",
    "    random.shuffle(rooms)\n",
    "    return rooms\n",
    "\n",
    "\n",
    "def get_room_connections(n_rooms, ask_for_connections, connection_type):\n",
    "    \"\"\"Generates connection edges between rooms\"\"\"\n",
    "    if not ask_for_connections:\n",
    "        return list(generate_random_connected_graph(n_rooms).edges)\n",
    "\n",
    "    if connection_type == 0:  # fully connected\n",
    "        return [(a, b) for a in range(n_rooms) for b in range(a + 1, n_rooms)]\n",
    "\n",
    "    elif connection_type == 1:  # star-shaped\n",
    "        central = random.randint(0, n_rooms - 1)\n",
    "        return [(central, i) for i in range(n_rooms) if i != central]\n",
    "\n",
    "    elif connection_type == 2:  # circular\n",
    "        edges = [(i, i + 1) for i in range(n_rooms - 1)]\n",
    "        if n_rooms > 2:\n",
    "            edges.append((0, n_rooms - 1))\n",
    "        return edges\n",
    "\n",
    "    elif connection_type == 3:  # linear\n",
    "        return [(i, i + 1) for i in range(n_rooms - 1)]\n",
    "\n",
    "    else:  # fallback: random connected\n",
    "        return list(generate_random_connected_graph(n_rooms).edges)\n",
    "\n",
    "\n",
    "def get_asset_placements(n_rooms, ask_for_placements, n_assets):\n",
    "    \"\"\"Returns a list of (room_index, asset_name) tuples\"\"\"\n",
    "    if not ask_for_placements:\n",
    "        return []\n",
    "\n",
    "    selected_rooms = random.choices(range(n_rooms), k=n_assets)\n",
    "    if n_assets > 1:\n",
    "        half = n_assets // 2\n",
    "        selected_assets = random.sample(AIRCHITECT_ASSETS, k=half) + \\\n",
    "                          random.sample(AIRCHITECT_ASSETS, k=n_assets - half)\n",
    "    else:\n",
    "        selected_assets = random.sample(AIRCHITECT_ASSETS, 1)\n",
    "\n",
    "    return list(zip(selected_rooms, selected_assets))\n",
    "\n",
    "\n",
    "def generate_dataset(file_path, dataset_size):\n",
    "    \"\"\"Generates a dataset and writes it to a file\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        data = []\n",
    "\n",
    "    while len(data) < dataset_size:\n",
    "        ask_for_rooms = bool(random.getrandbits(1))\n",
    "        n_rooms = random.randint(1, 8)\n",
    "\n",
    "        ask_for_connections = bool(random.getrandbits(1))\n",
    "        connection_type = random.randint(0, 6)\n",
    "\n",
    "        ask_for_placements = bool(random.getrandbits(1))\n",
    "        n_assets = random.randint(1, 6)\n",
    "\n",
    "        rooms = get_room_list(n_rooms, ask_for_rooms)\n",
    "        edges = get_room_connections(n_rooms, ask_for_connections, connection_type)\n",
    "        assets = get_asset_placements(n_rooms, ask_for_placements, n_assets)\n",
    "\n",
    "        data.append({\n",
    "            \"output\": {\n",
    "                \"rooms\": rooms,\n",
    "                \"connections\": edges,\n",
    "                \"assets\": assets\n",
    "            },\n",
    "            \"params\": {\n",
    "                \"n_rooms\": n_rooms,\n",
    "                \"connection_type\": connection_type,\n",
    "                \"n_placements\": n_assets,\n",
    "                \"ask_for_rooms\": ask_for_rooms,\n",
    "                \"ask_for_connections\": ask_for_connections,\n",
    "                \"ask_for_placements\": ask_for_placements\n",
    "            }\n",
    "        })\n",
    "\n",
    "        if len(data) % 50 == 0:\n",
    "            print(f\"Generated: {len(data)}\")\n",
    "\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "\n",
    "generate_dataset(\"01_dataset.json\", dataset_size=40000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 02 – Split the Dataset into 8 Batches Based on Boolean Flags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Single input file (matches new dataset generation)\n",
    "INPUT_FILE = \"01_dataset.json\"\n",
    "\n",
    "\n",
    "def load_dataset(path):\n",
    "    \"\"\"Loads a JSON file and returns the data.\"\"\"\n",
    "    with open(path, \"r\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "\n",
    "def assign_batches(dataset):\n",
    "    \"\"\"\n",
    "    Groups dataset entries into 8 batches based on the presence of\n",
    "    ask_for_rooms, ask_for_connections, and ask_for_placements.\n",
    "    \"\"\"\n",
    "    batches = {i: [] for i in range(8)}\n",
    "\n",
    "    for entry in dataset:\n",
    "        params = entry[\"params\"]\n",
    "        key = (\n",
    "            params[\"ask_for_rooms\"],\n",
    "            params[\"ask_for_connections\"],\n",
    "            params[\"ask_for_placements\"],\n",
    "        )\n",
    "\n",
    "        # Convert (bool, bool, bool) to integer 0–7\n",
    "        batch_index = sum(val * (2 ** idx) for idx, val in enumerate(reversed(key)))\n",
    "        batches[batch_index].append(entry)\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "def save_batches(batches):\n",
    "    \"\"\"Saves each batch to a separate JSON file.\"\"\"\n",
    "    for batch_index, entries in batches.items():\n",
    "        filename = f\"02_batch{batch_index:02}.json\"\n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump(entries, f, indent=4)\n",
    "\n",
    "        print(f\"Saved {filename} with {len(entries)} entries.\")\n",
    "\n",
    "\n",
    "def process_file(input_file):\n",
    "    \"\"\"Main processing flow for single input file.\"\"\"\n",
    "    input_path = Path(input_file)\n",
    "    dataset = load_dataset(input_path)\n",
    "    batches = assign_batches(dataset)\n",
    "    save_batches(batches)\n",
    "\n",
    "\n",
    "# === Entry Point ===\n",
    "process_file(INPUT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step03 - **Step 03 – Generate Natural Language Inputs from Structured Outputs****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ast\n",
    "import random\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"\")\n",
    "MODEL = \"gpt-4o\"\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"  \n",
    "You are an AI assistant designed to generate a dataset for the training process of a Language Model.\n",
    "The dataset I want to create consists of input, output pairs where the inputs are user prompts describing a flat layout in natural language. The outputs are lists which represent the requested floor plan from the user input as structured data.\n",
    "**input**\n",
    "The input is a string like \"Generate a house with 4 rooms. Add a toilet into the bathroom and a bed into the bedroom.\" or \"A space with 4 rooms. Add a bath and a living room.\"\n",
    "**output**\n",
    "The output consists of the following keys:\n",
    "- rooms: This is the list of rooms that the user asked for, example: ['Living Room', 'Bedroom', 'Bathroom']\n",
    "- connections: This is the list which describes whether two rooms should be connected or not. Example: [(0,1),(0,2),(2,3)]  - the tuple (0,1) means that the rooms at index 0 and index 1 from the \"rooms\" list should be connected.\n",
    "- assets: This is the list which stores the placement of assets, that the user specifically asked for. Example: [(0,'Toilet'),(2,'Sofa')]  - the tuple (2,'Sofa') as example means that the user requested a sofa to be in room at index 2 from the rooms list\n",
    "\n",
    "Your task is to generate possible user inputs for given outputs.\n",
    "Important:\n",
    "- If there are assets in the assets list, user inputs should ask for those assets explicitly to be placed into the correct room.\n",
    "- If there are no assets in the assets list, users should not ask for assets to be placed.\n",
    "- Generate exactly one user input for every given output in the list. Not more, not less.\n",
    "\n",
    "Return the generated inputs consisting of 30 entries as a list of strings like, the formatting is important: \n",
    "Just this string, no json formatting: ['String 1', 'String 2', ...]\n",
    "\"\"\"\n",
    "\n",
    "def get_style():\n",
    "    x = random.randint(1, 9)\n",
    "    if x <= 5:\n",
    "        return \"\"\n",
    "    elif x == 6:\n",
    "        return \"If there are assets to be place, user prompts should ask for the placements first, then for the rooms and connections.\"\n",
    "    elif x == 7:\n",
    "        return \"User prompts should not start with a verb.\"\n",
    "    elif x == 8:\n",
    "        return \"User prompts should not include any verbs.\"\n",
    "    elif x == 9:\n",
    "        return \"User prompts should be grammatically wrong.\"\n",
    "\n",
    "\n",
    "def generate_inputs(outputs, batch):\n",
    "    user_prompt = \"\"\"  \n",
    "    Generate possible user inputs for the following inputs. Return them as a list.\n",
    "    Follow these rules:\n",
    "    - Generate exactly one user input for every given output in the list. \n",
    "    - Use variant ways to describe the layouts but focus on simple language. \\n\n",
    "    \"\"\" + str(get_style()) + \"\\n\"\n",
    "\n",
    "    if batch >= 4:\n",
    "        user_prompt += \"- User inputs should request the specific room types, that should be used.\\n\"\n",
    "    else:\n",
    "        user_prompt += \"- User inputs should not ask for specific room types. Instead they should state something like 'Generate a house with 4 rooms ...\\n'\"\n",
    "\n",
    "    if batch in [2, 3, 6, 7]:\n",
    "        user_prompt += \"\"\" \n",
    "    - User inputs should ask for specific room connections. Use one of those list if it matches the given output: \n",
    "        - ... connected to ... / ... connected to ... and ... connected to ...  (and ...)\n",
    "        - all rooms connected to ... \n",
    "        - fully connected layout (every room has connection to each other room) \n",
    "        - circular connections (example for 4 rooms: [(0,1),(0,3),(1,2),(2,3)])\n",
    "        - linear connections (example for 3 rooms: [(0,1),(1,2),(2,3)])\n",
    "    \"\"\"\n",
    "    else:\n",
    "        user_prompt += \"- User inputs should not ask for specific room connections.\\n\"\n",
    "\n",
    "    if batch % 2 == 1:\n",
    "        user_prompt += \"- User inputs should explicitly ask for all asset placements.\\n\"\n",
    "    else:\n",
    "        user_prompt += \"- User inputs should not ask for asset placements.\\n\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": user_prompt + \"Outputs: \\n\" + str(outputs)}\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def generate(input_file, output_file, batch_size, batch):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    try:\n",
    "        with open(output_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            completed = json.load(file)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        completed = []\n",
    "\n",
    "    i = len(completed)\n",
    "    print(i)\n",
    "\n",
    "    while (i + batch_size) < len(data):\n",
    "        outputs = []\n",
    "        for j in range(batch_size):\n",
    "            entry = data[i + j]\n",
    "            outputs.append(str(entry[\"output\"]))\n",
    "            entry[\"input\"] = str(entry[\"output\"])\n",
    "\n",
    "        print(len(outputs))\n",
    "        response = generate_inputs(outputs, batch)\n",
    "        print(response)\n",
    "\n",
    "        try:\n",
    "            parsed_inputs = ast.literal_eval(response)\n",
    "            print(len(parsed_inputs))\n",
    "\n",
    "            if len(parsed_inputs) == 30:\n",
    "                new_entries = []\n",
    "                for user_input, output in zip(parsed_inputs, outputs):\n",
    "                    new_entries.append({\n",
    "                        \"input\": user_input,\n",
    "                        \"output\": output\n",
    "                    })\n",
    "\n",
    "                completed += new_entries\n",
    "\n",
    "                with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "                    json.dump(completed, file, indent=4)\n",
    "\n",
    "                print(f\"Modified JSON data saved to {output_file}\")\n",
    "                i += batch_size\n",
    "            else:\n",
    "                print(\"Bad number of generated inputs.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"That did not work!\")\n",
    "            print(e)\n",
    "\n",
    "\n",
    "# === Run for All 8 Batches ===\n",
    "batch_size = 30\n",
    "\n",
    "for batch in range(8):\n",
    "    input_file = f\"02_batch{batch:02}.json\"\n",
    "    output_file = f\"03_batch{batch:02}.json\"\n",
    "    generate(input_file, output_file, batch_size, batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 04 – Merge All Batches into a Single Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def save_batches():\n",
    "    output_file = \"04_dataset.json\"\n",
    "\n",
    "    # Load existing data if present\n",
    "    try:\n",
    "        with open(output_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            completed = json.load(file)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        completed = []\n",
    "\n",
    "    # Process all 8 batches\n",
    "    for batch in range(8):\n",
    "        input_file = f\"03_batch{str(batch).zfill(2)}.json\"\n",
    "        if not os.path.exists(input_file):\n",
    "            print(f\"Warning: {input_file} does not exist. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        completed += data\n",
    "\n",
    "    # Save merged result\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(completed, file, indent=4)\n",
    "\n",
    "    print(f\"Saved: {output_file} with {len(completed)} entries.\")\n",
    "\n",
    "\n",
    "# === Merge Batches ===\n",
    "save_batches()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
