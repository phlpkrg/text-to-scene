{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Collection of Processing Steps For Deriving Dataset Variants From Base Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding Room Identifiers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def load_dataset(path):\n",
    "    \"\"\"Loads a JSON file and returns the data.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "\n",
    "def number_rooms(rooms):\n",
    "    \"\"\"\n",
    "    Appends a counter to duplicate room names.\n",
    "    Example: ['Bathroom', 'Bathroom'] â†’ ['Bathroom 1', 'Bathroom 2']\n",
    "    \"\"\"\n",
    "    room_counts = {}\n",
    "    numbered = []\n",
    "\n",
    "    for room in rooms:\n",
    "        room_counts[room] = room_counts.get(room, 0) + 1\n",
    "        numbered.append(f\"{room} {room_counts[room]}\")\n",
    "\n",
    "    return numbered\n",
    "\n",
    "\n",
    "def process_entries(dataset):\n",
    "    \"\"\"Processes all entries, numbering duplicate room types.\"\"\"\n",
    "    for entry in dataset:\n",
    "        output_data = json.loads(entry[\"output\"].replace(\"'\", '\"'))\n",
    "        output_data[\"rooms\"] = number_rooms(output_data[\"rooms\"])\n",
    "        entry[\"output\"] = json.dumps(output_data).replace('\"', \"'\")\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def save_dataset(path, data):\n",
    "    \"\"\"Saves the processed dataset to the output path.\"\"\"\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "    print(f\"Saved {path.name} with {len(data)} entries.\")\n",
    "\n",
    "\n",
    "def process_file(input_file, output_file):\n",
    "    \"\"\"Main processing flow for numbering room types.\"\"\"\n",
    "    input_path = Path(input_file)\n",
    "    output_path = Path(output_file)\n",
    "\n",
    "    dataset = load_dataset(input_path)\n",
    "    processed = process_entries(dataset)\n",
    "    save_dataset(output_path, processed)\n",
    "\n",
    "\n",
    "# === Entry Point ===\n",
    "INPUT_FILE = \"your_input_file.json\"\n",
    "OUTPUT_FILE = \"your_output_file.json\"\n",
    "process_file(INPUT_FILE, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quantifying Asset Placements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "NUMBER_WORDS = [\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\"]\n",
    "\n",
    "def load_dataset(path):\n",
    "    \"\"\"Loads a JSON file and returns the data.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "\n",
    "def save_dataset(path, data):\n",
    "    \"\"\"Saves the dataset to the given path.\"\"\"\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "    print(f\"Saved {path.name} with {len(data)} entries.\")\n",
    "\n",
    "\n",
    "def select_for_augmentation(dataset, percentage):\n",
    "    \"\"\"\n",
    "    Randomly selects a percentage of entries for augmentation.\n",
    "    Returns a tuple: (entries_to_modify, entries_to_keep)\n",
    "    \"\"\"\n",
    "    n_total = len(dataset)\n",
    "    n_selected = int(n_total * percentage)\n",
    "    selected = random.sample(dataset, n_selected)\n",
    "    selected_set = set(map(str, selected))\n",
    "    remaining = [entry for entry in dataset if str(entry) not in selected_set]\n",
    "    return selected, remaining\n",
    "\n",
    "\n",
    "def modify_entry(entry):\n",
    "    \"\"\"\n",
    "    Modifies an entry by duplicating an asset in a room and pluralizing its mention in the input.\n",
    "    \"\"\"\n",
    "    input_text = entry[\"input\"]\n",
    "    output_data = entry[\"output\"]\n",
    "    room_assets = {}\n",
    "\n",
    "    matches = re.findall(r\"([\\w\\s]+)=\\[(.*?)\\]\", output_data)\n",
    "    for room, assets_str in matches:\n",
    "        room = room.strip().lstrip(\",\")\n",
    "        assets = [asset.strip().strip(\"'\") for asset in assets_str.split(\"', '\") if asset]\n",
    "        room_assets[room] = assets\n",
    "\n",
    "    valid_rooms = [room for room, assets in room_assets.items() if assets]\n",
    "    if not valid_rooms:\n",
    "        return entry\n",
    "\n",
    "    chosen_room = random.choice(valid_rooms)\n",
    "    chosen_asset = random.choice(room_assets[chosen_room])\n",
    "\n",
    "    x = random.randint(2, 10)\n",
    "    x_str = str(x) if random.random() < 0.5 else NUMBER_WORDS[x]\n",
    "\n",
    "    asset_list = room_assets[chosen_room]\n",
    "    if chosen_asset in asset_list:\n",
    "        index = asset_list.index(chosen_asset)\n",
    "        asset_list[index + 1:index + 1] = [chosen_asset] * (x - 1)\n",
    "\n",
    "    modified_output = \", \".join(\n",
    "        f\"{room}={str(room_assets[room])}\" for room in room_assets\n",
    "    )\n",
    "\n",
    "    plural_asset = chosen_asset + \"s\"\n",
    "    regex_pattern = rf\"\\b(?:a|an)?\\b\\s*{re.escape(chosen_asset)}\\b\"\n",
    "\n",
    "    def case_insensitive_replace(match):\n",
    "        preceding = match.string[match.start() - 1] if match.start() > 0 else \" \"\n",
    "        space = \"\" if preceding.isspace() else \" \"\n",
    "        return f\"{space}{x_str} {plural_asset}\"\n",
    "\n",
    "    input_text = re.sub(regex_pattern, case_insensitive_replace, input_text, count=1, flags=re.IGNORECASE)\n",
    "\n",
    "    return {\n",
    "        \"input\": input_text,\n",
    "        \"output\": modified_output\n",
    "    }\n",
    "\n",
    "\n",
    "def process_file(input_file, output_file, percentage):\n",
    "    \"\"\"Main processing flow: randomly modify a percentage of entries and merge results.\"\"\"\n",
    "    input_path = Path(input_file)\n",
    "    output_path = Path(output_file)\n",
    "\n",
    "    dataset = load_dataset(input_path)\n",
    "    to_modify, remaining = select_for_augmentation(dataset, percentage)\n",
    "    modified = [modify_entry(entry) for entry in to_modify]\n",
    "    merged = modified + remaining\n",
    "    save_dataset(output_path, merged)\n",
    "\n",
    "\n",
    "# === Run Augmentation ===\n",
    "INPUT_FILE = \"your_input_file.json\"\n",
    "OUTPUT_FILE = \"your_output_file.json\"\n",
    "AUGMENTATION_PERCENTAGE = 0.15  # e.g. 0.15 = 15% of entries will be modified\n",
    "process_file(INPUT_FILE, OUTPUT_FILE, AUGMENTATION_PERCENTAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entry Duplication**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def load_dataset(path):\n",
    "    \"\"\"Loads a JSON file and returns the data.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "\n",
    "def save_dataset(path, data):\n",
    "    \"\"\"Saves the dataset to the output path.\"\"\"\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "    print(f\"Saved {path.name} with {len(data)} entries.\")\n",
    "\n",
    "\n",
    "def duplicate_entries(dataset, times=4):\n",
    "    \"\"\"Duplicates the dataset N times.\"\"\"\n",
    "    return dataset * times\n",
    "\n",
    "\n",
    "def process_file(input_file, output_file, times=4):\n",
    "    \"\"\"Main processing flow: duplicate dataset entries and save to output.\"\"\"\n",
    "    input_path = Path(input_file)\n",
    "    output_path = Path(output_file)\n",
    "\n",
    "    dataset = load_dataset(input_path)\n",
    "    duplicated = duplicate_entries(dataset, times=times)\n",
    "    save_dataset(output_path, duplicated)\n",
    "\n",
    "\n",
    "# === Run Duplication ===\n",
    "INPUT_FILE = \"your_input_file.json\"\n",
    "OUTPUT_FILE = \"your_output_file.json\"\n",
    "process_file(INPUT_FILE, OUTPUT_FILE, times=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Asset Substitution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "# === Ensure WordNet is available ===\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "def load_dataset(path):\n",
    "    \"\"\"Loads a JSON file and returns the data.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "\n",
    "def save_dataset(path, data):\n",
    "    \"\"\"Saves the dataset to the given path.\"\"\"\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "    print(f\"Saved {path.name} with {len(data)} entries.\")\n",
    "\n",
    "\n",
    "def is_exclusively_noun(word):\n",
    "    \"\"\"Checks if all synsets of the word are nouns.\"\"\"\n",
    "    synsets = wordnet.synsets(word)\n",
    "    return synsets and all(s.pos() == 'n' for s in synsets)\n",
    "\n",
    "\n",
    "def get_unique_nouns():\n",
    "    \"\"\"Returns a set of unique lowercase single-word nouns.\"\"\"\n",
    "    nouns = set()\n",
    "    for synset in wordnet.all_synsets('n'):\n",
    "        for lemma in synset.lemma_names():\n",
    "            if \"_\" not in lemma:\n",
    "                word = lemma.lower()\n",
    "                if is_exclusively_noun(word):\n",
    "                    nouns.add(word)\n",
    "    return nouns\n",
    "\n",
    "\n",
    "def get_unique_replacement(existing_replacements, original_asset, nouns):\n",
    "    \"\"\"Finds a unique replacement noun not too similar to others.\"\"\"\n",
    "    while True:\n",
    "        replacement = random.choice(list(nouns))\n",
    "\n",
    "        if replacement.lower() == original_asset.lower():\n",
    "            continue\n",
    "\n",
    "        if any(replacement.lower() in existing.lower() or existing.lower() in replacement.lower()\n",
    "               for existing in existing_replacements.values()):\n",
    "            continue\n",
    "\n",
    "        return replacement\n",
    "\n",
    "\n",
    "def process_entries(dataset, nouns):\n",
    "    \"\"\"Processes each entry by replacing assets with random WordNet nouns.\"\"\"\n",
    "    for entry in dataset:\n",
    "        output_text = entry[\"output\"]\n",
    "        input_text = entry[\"input\"]\n",
    "\n",
    "        assets_found = re.findall(r\"'([^']*)'\", output_text)\n",
    "        asset_replacement_map = {}\n",
    "\n",
    "        for asset in assets_found:\n",
    "            replacement = get_unique_replacement(asset_replacement_map, asset.lower(), nouns)\n",
    "            replacement = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', replacement).lower()\n",
    "            asset_replacement_map[asset] = replacement\n",
    "\n",
    "        # Replace in output\n",
    "        for original, replacement in asset_replacement_map.items():\n",
    "            output_text = output_text.replace(f\"'{original}'\", f\"'{replacement}'\")\n",
    "\n",
    "        # Replace in input\n",
    "        for original, replacement in asset_replacement_map.items():\n",
    "            input_text = re.sub(\n",
    "                rf\"\\b{re.escape(original)}(s?)\\b\",\n",
    "                lambda match: replacement + match.group(1),\n",
    "                input_text,\n",
    "                flags=re.IGNORECASE\n",
    "            )\n",
    "\n",
    "        entry[\"output\"] = output_text\n",
    "        entry[\"input\"] = input_text\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def process_file(input_file, output_file):\n",
    "    \"\"\"Main processing function for replacing assets with random WordNet nouns.\"\"\"\n",
    "    input_path = Path(input_file)\n",
    "    output_path = Path(output_file)\n",
    "\n",
    "    dataset = load_dataset(input_path)\n",
    "    nouns = get_unique_nouns()\n",
    "    processed = process_entries(dataset, nouns)\n",
    "    save_dataset(output_path, processed)\n",
    "\n",
    "\n",
    "# === Entry Point ===\n",
    "INPUT_FILE = \"your_input_file.json\"\n",
    "OUTPUT_FILE = \"your_output_file.json\"\n",
    "process_file(INPUT_FILE, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating Available Asset List**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "\n",
    "def load_dataset(path):\n",
    "    \"\"\"Loads a JSON file and returns the data.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "\n",
    "def save_dataset(path, data):\n",
    "    \"\"\"Saves the dataset to the given path.\"\"\"\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "    print(f\"Saved {path.name} with {len(data)} entries.\")\n",
    "\n",
    "\n",
    "def is_exclusively_noun(word):\n",
    "    \"\"\"Checks if all synsets of the word are nouns.\"\"\"\n",
    "    synsets = wordnet.synsets(word)\n",
    "    return all(s.pos() == 'n' for s in synsets)\n",
    "\n",
    "\n",
    "def get_unique_nouns():\n",
    "    \"\"\"Returns a list of unique lowercase single-word nouns.\"\"\"\n",
    "    nouns = set()\n",
    "    for synset in wordnet.all_synsets('n'):\n",
    "        for lemma in synset.lemma_names():\n",
    "            if \"_\" not in lemma:\n",
    "                word = lemma.lower()\n",
    "                if is_exclusively_noun(word):\n",
    "                    nouns.add(word)\n",
    "    return list(nouns)\n",
    "\n",
    "\n",
    "def normalize_asset(asset):\n",
    "    \"\"\"Strips quotes and converts asset to lowercase.\"\"\"\n",
    "    return asset.strip().strip(\"'\").strip('\"').lower()\n",
    "\n",
    "\n",
    "def extract_assets(output_text):\n",
    "    \"\"\"Extracts asset names from the output string.\"\"\"\n",
    "    assets = set()\n",
    "    matches = re.findall(r\"\\[(.*?)\\]\", output_text)\n",
    "    for match in matches:\n",
    "        if match.strip():\n",
    "            extracted = [normalize_asset(asset) for asset in match.split(\", \") if asset.strip()]\n",
    "            assets.update(extracted)\n",
    "    return list(assets)\n",
    "\n",
    "\n",
    "def generate_random_assets(existing_assets, noun_list):\n",
    "    \"\"\"\n",
    "    Generates a randomized 'available' asset list.\n",
    "    - May include all or a subset of the placed assets.\n",
    "    - Fills the rest with unique nouns.\n",
    "    \"\"\"\n",
    "    if random.random() < 0.5:\n",
    "        included = existing_assets\n",
    "    else:\n",
    "        included = random.sample(existing_assets, k=random.randint(0, len(existing_assets))) if existing_assets else []\n",
    "\n",
    "    total_size = random.randint(1, 30) if random.random() < 0.5 else random.randint(30, 50)\n",
    "    filler = random.sample(noun_list, k=max(0, total_size - len(included)))\n",
    "\n",
    "    final_assets = list(set(included)) + filler\n",
    "    random.shuffle(final_assets)\n",
    "\n",
    "    return sorted(set(normalize_asset(asset) for asset in final_assets))[:total_size]\n",
    "\n",
    "\n",
    "def process_entries(dataset, nouns):\n",
    "    \"\"\"Processes each entry by adding available assets and handling unknowns.\"\"\"\n",
    "    for entry in dataset:\n",
    "        output = entry.get(\"output\", \"\")\n",
    "        placed_assets = extract_assets(output)\n",
    "        available_assets = generate_random_assets(placed_assets, nouns)\n",
    "        available_set = {normalize_asset(asset) for asset in available_assets}\n",
    "\n",
    "        modified_output = []\n",
    "        room_entries = re.findall(r\"([\\w\\s]+\\d*)=\\[(.*?)\\]\", output)\n",
    "\n",
    "        for room, assets_str in room_entries:\n",
    "            assets = [normalize_asset(asset) for asset in assets_str.split(\", \") if asset.strip()]\n",
    "            assets_with_unknowns = [\n",
    "                f\"'{asset}'\" if asset in available_set else \"'Unknown'\"\n",
    "                for asset in assets\n",
    "            ]\n",
    "            modified_output.append(f\"{room}=[{', '.join(assets_with_unknowns)}]\")\n",
    "\n",
    "        available_str = f\", available=[{', '.join(f'\\'{asset}\\'' for asset in available_assets)}]\"\n",
    "        entry[\"input\"] = entry[\"input\"].rstrip(\"]\") + \"]\" + available_str\n",
    "        entry[\"output\"] = \", \".join(modified_output)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def process_file(input_file, output_file):\n",
    "    \"\"\"Main processing function to inject available assets and mark unknowns.\"\"\"\n",
    "    input_path = Path(input_file)\n",
    "    output_path = Path(output_file)\n",
    "\n",
    "    dataset = load_dataset(input_path)\n",
    "    nouns = get_unique_nouns()\n",
    "    processed = process_entries(dataset, nouns)\n",
    "    save_dataset(output_path, processed)\n",
    "\n",
    "\n",
    "# === Entry Point ===\n",
    "INPUT_FILE = \"your_input_file.json\"\n",
    "OUTPUT_FILE = \"your_output_file.json\"\n",
    "process_file(INPUT_FILE, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Randomized Ordering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def load_dataset(path):\n",
    "    \"\"\"Loads a JSON file and returns the data.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "\n",
    "def save_dataset(path, data):\n",
    "    \"\"\"Saves the dataset to the given path.\"\"\"\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "    print(f\"Saved {path.name} with {len(data)} entries.\")\n",
    "\n",
    "\n",
    "def shuffle_dataset(data):\n",
    "    \"\"\"Shuffles the dataset entries in-place and returns them.\"\"\"\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "def process_file(input_file, output_file):\n",
    "    \"\"\"Main processing function to shuffle dataset entries.\"\"\"\n",
    "    input_path = Path(input_file)\n",
    "    output_path = Path(output_file)\n",
    "    dataset = load_dataset(input_path)\n",
    "\n",
    "    shuffled = shuffle_dataset(dataset)\n",
    "    save_dataset(output_path, shuffled)\n",
    "\n",
    "\n",
    "# === Entry Point ===\n",
    "INPUT_FILE = \"your_input_file.json\"\n",
    "OUTPUT_FILE = \"your_output_file.json\"\n",
    "process_file(INPUT_FILE, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Partitioning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def load_dataset(path):\n",
    "    \"\"\"Loads a JSON file and returns the data.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "\n",
    "def save_dataset(path, data):\n",
    "    \"\"\"Saves the dataset to the given path.\"\"\"\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "    print(f\"Saved {path.name} with {len(data)} entries.\")\n",
    "\n",
    "\n",
    "def split_dataset(data):\n",
    "    \"\"\"\n",
    "    Splits the dataset into two equal parts (order preserved).\n",
    "    Returns a tuple: (first_half, second_half)\n",
    "    \"\"\"\n",
    "    half = len(data) // 2\n",
    "    return data[:half], data[half:half * 2]\n",
    "\n",
    "\n",
    "def process_file(input_file, output_file_1, output_file_2):\n",
    "    \"\"\"Main processing function to split dataset into two batches without shuffling.\"\"\"\n",
    "    input_path = Path(input_file)\n",
    "    output_path_1 = Path(output_file_1)\n",
    "    output_path_2 = Path(output_file_2)\n",
    "\n",
    "    dataset = load_dataset(input_path)\n",
    "    part1, part2 = split_dataset(dataset)\n",
    "\n",
    "    save_dataset(output_path_1, part1)\n",
    "    save_dataset(output_path_2, part2)\n",
    "\n",
    "\n",
    "# === Entry Point ===\n",
    "INPUT_FILE = \"your_input_file.json\"\n",
    "OUTPUT_FILE_1 = \"your_output_file1.json\"\n",
    "OUTPUT_FILE_2 = \"your_output_file2.json\"\n",
    "process_file(INPUT_FILE, OUTPUT_FILE_1, OUTPUT_FILE_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
